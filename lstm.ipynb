{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Layer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# داده‌های ساده برای مثال (جایگزین با دیتای واقعی خودت کن)\n",
        "corpus = [\"I love this movie\", \"This film was terrible\", \"Amazing performance\", \"I hate the plot\", \"Best experience\"]\n",
        "labels = np.array([1, 0, 1, 0, 1])  # 1: مثبت، 0: منفی\n",
        "\n",
        "# پردازش متن\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "text_sequences = tokenizer.texts_to_sequences(corpus)\n",
        "max_len = max(len(seq) for seq in text_sequences)\n",
        "text_pad = pad_sequences(text_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "vocab_length = len(tokenizer.word_index) + 1  # +1 برای توکن صفر\n",
        "\n",
        "# تعریف لایه Attention سفارشی\n",
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)\n",
        "        at = K.softmax(et)\n",
        "        at = K.expand_dims(at, axis=-1)\n",
        "        output = x * at\n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n",
        "\n",
        "# ساخت مدل\n",
        "inputs = Input(shape=(max_len,))\n",
        "x = Embedding(input_dim=vocab_length, output_dim=32, input_length=max_len)(inputs)\n",
        "x = LSTM(100, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)(x)\n",
        "x = Attention()(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# آموزش مدل\n",
        "model.fit(text_pad, labels, batch_size=2, epochs=10, validation_split=0.2, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3gw7TH3FtqE",
        "outputId": "8a16e9ca-7d76-49b0-d37f-045457f338f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 972ms/step - accuracy: 0.1667 - loss: 0.6941 - val_accuracy: 1.0000 - val_loss: 0.6918\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3333 - loss: 0.6942 - val_accuracy: 0.0000e+00 - val_loss: 0.6938\n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8333 - loss: 0.6895 - val_accuracy: 0.0000e+00 - val_loss: 0.6944\n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8333 - loss: 0.6880 - val_accuracy: 0.0000e+00 - val_loss: 0.6936\n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6667 - loss: 0.6916 - val_accuracy: 1.0000 - val_loss: 0.6921\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8333 - loss: 0.6846 - val_accuracy: 1.0000 - val_loss: 0.6914\n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8333 - loss: 0.6820 - val_accuracy: 1.0000 - val_loss: 0.6912\n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.6667 - loss: 0.6842 - val_accuracy: 1.0000 - val_loss: 0.6895\n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.6786 - val_accuracy: 1.0000 - val_loss: 0.6880\n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6667 - loss: 0.6802 - val_accuracy: 1.0000 - val_loss: 0.6858\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bc3a1867dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}